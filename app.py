import streamlit as st
import pandas as pd
import os
import glob
import hashlib
import base64
from datetime import datetime
from dotenv import load_dotenv

# --- å®‰å®šæ€§ã‚’é‡è¦–ã—ãŸæœ€æ–°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
# from langchain_community.vectorstores import Chroma
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ğŸ’¡ ãƒã‚¤ãƒ³ãƒˆï¼šã‚¨ãƒ©ãƒ¼ã®å…ƒã¨ãªã‚‹ langchain.chains é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’å®Œå…¨ã«æ’é™¤ã—ã¾ã—ãŸ

# 1. ç’°å¢ƒè¨­å®š
load_dotenv()
st.set_page_config(page_title="éŠ€è¡Œæ‰‹ç¶šDXãƒãƒ¼ã‚¿ãƒ«", layout="wide")

# ãƒ‘ã‚¹è¨­å®š
DB_FILE = "bank_document_index.xlsx"
DATA_DIR = "./clients_data"
CHROMA_DIR = "./chroma_db"

# ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---

def get_file_hash(path):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆå¤‰æ›´æ¤œçŸ¥ç”¨ï¼‰"""
    hasher = hashlib.md5()
    with open(path, 'rb') as f:
        hasher.update(f.read())
    return hasher.hexdigest()

def get_pdf_display_link(file_path):
    """PDFã‚’Streamlitä¸Šã«åŸ‹ã‚è¾¼ã‚€ãŸã‚ã®HTMLç”Ÿæˆ"""
    with open(file_path, "rb") as f:
        base64_pdf = base64.b64encode(f.read()).decode('utf-8')
    return f'<embed src="data:application/pdf;base64,{base64_pdf}" width="100%" height="800" type="application/pdf">'

# --- ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ï¼šãƒ‡ãƒ¼ã‚¿åŒæœŸ ---

def sync_data():
    st.info("ã‚¹ã‚­ãƒ£ãƒ³ã‚’é–‹å§‹ã—ã¾ã™ã€‚ã‚¹ã‚­ãƒ£ãƒ³ç”»åƒã‚’å«ã‚€ãŸã‚ã€AIã®è¦–è¦šè§£æãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™...")
    
    # 1. å…¨PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
    pdf_files = glob.glob(os.path.join(DATA_DIR, "**/*.pdf"), recursive=True)
    
    # 2. æ—¢å­˜å°å¸³ã®èª­ã¿è¾¼ã¿
    if os.path.exists(DB_FILE):
        df_db = pd.read_excel(DB_FILE)
    else:
        df_db = pd.DataFrame(columns=["ãƒ•ã‚¡ã‚¤ãƒ«å", "ãƒ•ãƒ«ãƒ‘ã‚¹", "ãƒãƒƒã‚·ãƒ¥", "éŠ€è¡Œ", "æ›¸é¡ç¨®åˆ¥", "æœ€çµ‚æ›´æ–°æ—¥"])

    # 3. è§£æãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ï¼ˆGemini 1.5 Flashã¯ç”»åƒ/PDFã‚’ç›´æ¥èª­ã‚ã¾ã™ï¼‰
    # APIã‚­ãƒ¼ã®é©ç”¨
    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
    model = genai.GenerativeModel("gemini-2.5-flash")
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

    # 4. ãƒ•ã‚¡ã‚¤ãƒ«ç²¾æŸ»
    for file_path in pdf_files:
        file_hash = get_file_hash(file_path)
        
        is_new = file_path not in df_db["ãƒ•ãƒ«ãƒ‘ã‚¹"].values
        is_modified = not is_new and df_db.loc[df_db["ãƒ•ãƒ«ãƒ‘ã‚¹"] == file_path, "ãƒãƒƒã‚·ãƒ¥"].values[0] != file_hash
        
        if is_new or is_modified:
            st.write(f"ğŸ‘ï¸ AIè¦–è¦šè§£æä¸­: {os.path.basename(file_path)}")
            
            try:
                # ã€é‡è¦ã€‘PDFã‚’Geminiã«ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã§ã‚¹ã‚­ãƒ£ãƒ³ç”»åƒã‚‚èª­ã‚ã‚‹ï¼‰
                # MIMEã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®šã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                uploaded_file = genai.upload_file(file_path, mime_type="application/pdf")
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼šJSONå½¢å¼ã§æƒ…å ±ã‚’æŠ½å‡ºã•ã›ã‚‹
                prompt = """
                ã“ã®éŠ€è¡Œæ‰‹ç¶šæ›¸é¡ã‚’è§£æã—ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
                å‡ºåŠ›ã¯å¿…ãšã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼ˆCSVå½¢å¼ï¼‰ã§è¡Œã£ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªæ–‡ç« ã¯ä¸è¦ã§ã™ã€‚
                
                ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
                éŠ€è¡Œå,æ›¸é¡ã®ç¨®é¡,æ›¸é¡ã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ(å…¨æ–‡ã®OCRçµæœ)
                
                ä¾‹:
                ä¸‰è±UFJéŠ€è¡Œ,æ®‹é«˜è¨¼æ˜æ›¸ç™ºè¡Œä¾é ¼æ›¸,è¢«ç›¸ç¶šäººã€‡ã€‡ã®æ®‹é«˜è¨¼æ˜ã‚’ä¾é ¼ã™ã‚‹æ›¸é¡ã€‚å®Ÿå°ãŒå¿…è¦ã€‚
                """
                
                # Geminiã«ç”»åƒã‚’è¦‹ã›ã¦å›ç­”ã•ã›ã‚‹
                response = model.generate_content([prompt, uploaded_file])
                
                # è§£æçµæœã®åˆ†å‰²
                parts = response.text.split(",", 2)
                if len(parts) >= 3:
                    bank = parts[0].strip()
                    doc_type = parts[1].strip()
                    summary_text = parts[2].strip()
                else:
                    bank, doc_type, summary_text = "è§£æã‚¨ãƒ©ãƒ¼", "è§£æã‚¨ãƒ©ãƒ¼", response.text

                # å°å¸³æ›´æ–°
                new_row = {
                    "ãƒ•ã‚¡ã‚¤ãƒ«å": os.path.basename(file_path),
                    "ãƒ•ãƒ«ãƒ‘ã‚¹": file_path,
                    "ãƒãƒƒã‚·ãƒ¥": file_hash,
                    "éŠ€è¡Œ": bank,
                    "æ›¸é¡ç¨®åˆ¥": doc_type,
                    "æœ€çµ‚æ›´æ–°æ—¥": datetime.now().strftime("%Y-%m-%d %H:%M")
                }
                df_db = df_db[df_db["ãƒ•ãƒ«ãƒ‘ã‚¹"] != file_path]
                df_db = pd.concat([df_db, pd.DataFrame([new_row])], ignore_index=True)
                
                # ã€é‡è¦ã€‘ã‚¹ã‚­ãƒ£ãƒ³ãƒ‡ãƒ¼ã‚¿ã ã¨PyPDFLoaderã§ã¯ãƒ†ã‚­ã‚¹ãƒˆãŒå–ã‚Œãªã„ãŸã‚ã€
                # GeminiãŒç›®ã§è¦‹ã¦æ›¸ãèµ·ã“ã—ãŸã€Œsummary_textã€ã‚’ãƒ™ã‚¯ã‚¿ãƒ¼DBã«å…¥ã‚Œã‚‹
                from langchain.schema import Document
                doc = Document(page_content=summary_text, metadata={"source": file_path, "bank": bank})
                
                # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã—ã¦ä¿å­˜
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
                splits = text_splitter.split_documents([doc])
                
                # æœ€æ–°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§è­¦å‘Šå¯¾å¿œæ¸ˆã¿
                from langchain_chroma import Chroma
                Chroma.from_documents(
                    documents=splits,
                    embedding=embeddings,
                    persist_directory=CHROMA_DIR
                )
                
            except Exception as e:
                st.error(f"è§£æå¤±æ•— ({os.path.basename(file_path)}): {e}")
            
    # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨ä¿å­˜
    df_db = df_db[df_db["ãƒ•ãƒ«ãƒ‘ã‚¹"].apply(os.path.exists)]
    df_db.to_excel(DB_FILE, index=False)
    return df_db

# --- ç”»é¢ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ ---

st.title("ğŸ¦ éŠ€è¡Œæ‰‹ç¶šãƒŠãƒ¬ãƒƒã‚¸å…±æœ‰ã‚·ã‚¹ãƒ†ãƒ ")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šç®¡ç†æ©Ÿèƒ½
with st.sidebar:
    st.header("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†")
    if st.button("ğŸ”„ ãƒ•ã‚©ãƒ«ãƒ€ã‚’åŒæœŸã—ã¦AIè§£æ"):
        df_result = sync_data()
        st.success("åŒæœŸãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        st.rerun()
    
    st.divider()
    st.caption("â€» clients_data ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨PDFã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¾ã™ã€‚")

# å°å¸³ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
if os.path.exists(DB_FILE):
    df = pd.read_excel(DB_FILE)
else:
    df = pd.DataFrame()

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
if not df.empty:
    tab1, tab2 = st.tabs(["ğŸ” æ›¸é¡æ¤œç´¢ãƒ»ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ğŸ’¬ AIæ›¸ãæ–¹ç›¸è«‡"])

    # --- TAB1: æ¤œç´¢ã¨ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ---
    with tab1:
        search_q = st.text_input("éŠ€è¡Œåã‚„æ›¸é¡åã€äººåã§æ¤œç´¢", placeholder="ä¾‹: ä¸‰è±UFJ æ®‹é«˜è¨¼æ˜")
        
        # æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        mask = df.apply(lambda row: search_q.lower() in str(row).lower(), axis=1)
        filtered_df = df[mask]
        
        if not filtered_df.empty:
            col_list, col_view = st.columns([1, 1.2])
            
            with col_list:
                st.write(f"æ¤œç´¢çµæœ: {len(filtered_df)} ä»¶")
                selected_file_name = st.selectbox("ç¢ºèªã™ã‚‹æ›¸é¡ã‚’é¸æŠ", filtered_df["ãƒ•ã‚¡ã‚¤ãƒ«å"].tolist())
                selected_row = filtered_df[filtered_df["ãƒ•ã‚¡ã‚¤ãƒ«å"] == selected_file_name].iloc[0]
                
                # ç°¡æ˜“å°å¸³ç·¨é›†æ©Ÿèƒ½
                st.info(f"ğŸ“ ãƒ‘ã‚¹: {selected_row['ãƒ•ãƒ«ãƒ‘ã‚¹']}")
                with st.expander("å°å¸³æƒ…å ±ã‚’ä¿®æ­£ã™ã‚‹"):
                    new_bank = st.text_input("éŠ€è¡Œå", selected_row["éŠ€è¡Œ"])
                    new_type = st.text_input("æ›¸é¡ç¨®åˆ¥", selected_row["æ›¸é¡ç¨®åˆ¥"])
                    if st.button("ä¿®æ­£ã‚’ä¿å­˜"):
                        df.loc[df["ãƒ•ãƒ«ãƒ‘ã‚¹"] == selected_row["ãƒ•ãƒ«ãƒ‘ã‚¹"], ["éŠ€è¡Œ", "æ›¸é¡ç¨®åˆ¥"]] = [new_bank, new_type]
                        df.to_excel(DB_FILE, index=False)
                        st.toast("ä¿®æ­£ã‚’å°å¸³ã«åæ˜ ã—ã¾ã—ãŸ")
            
            with col_view:
                st.markdown(get_pdf_display_link(selected_row["ãƒ•ãƒ«ãƒ‘ã‚¹"]), unsafe_allow_html=True)
        else:
            st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹æ›¸é¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    # --- TAB2: AIã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ ---
    with tab2:
        st.subheader("ğŸ¤– AIæ›¸ãæ–¹ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ")
        
        # ãƒ™ã‚¯ã‚¿ãƒ¼DBã®ãƒ­ãƒ¼ãƒ‰
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        if os.path.exists(CHROMA_DIR):
            vectorstore = Chroma(persist_directory=CHROMA_DIR, embedding_function=embeddings)
            chat_llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)

            # ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
            if "messages" not in st.session_state:
                st.session_state.messages = []

            for m in st.session_state.messages:
                with st.chat_message(m["role"]): st.markdown(m["content"])

            if user_input := st.chat_input("ä¾‹: ã“ã®éŠ€è¡Œã®æ®‹é«˜è¨¼æ˜ã§ã€ä»£ç†äººã®ä½æ‰€ã¯ã©ã“ã«æ›¸ã‘ã°ã„ã„ï¼Ÿ"):
                st.session_state.messages.append({"role": "user", "content": user_input})
                with st.chat_message("user"): st.markdown(user_input)

                with st.chat_message("assistant"):
                    with st.spinner("éå»ã®æ›¸é¡ã‚’ç…§åˆä¸­..."):
                        # ğŸ’¡ ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼šRetrievalQAãªã©ã®Chainã‚’ä½¿ã‚ãšã€æ‰‹å‹•ã§RAGã‚’å®Œçµ
                        # 1. é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢
                        docs = vectorstore.similarity_search(user_input, k=5)
                        context = "\n\n".join([d.page_content for d in docs])
                        
                        # 2. Geminiç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
                        full_prompt = f"""ã‚ãªãŸã¯è¡Œæ”¿æ›¸å£«ã®å®Ÿå‹™è£œåŠ©è€…ã§ã™ã€‚ä»¥ä¸‹ã®éå»äº‹ä¾‹ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’å‚è€ƒã«ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
                        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ãªã„æƒ…å ±ã¯ã€Œä¸æ˜ã€ã¨ç­”ãˆã€çŸ¥ã£ãŸã‹ã¶ã‚Šã‚’ã—ãªã„ã§ãã ã•ã„ã€‚
                        
                        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
                        {context}
                        
                        è³ªå•: {user_input}"""
                        
                        # 3. ç›´æ¥LLMã‚’å‘¼ã³å‡ºã™
                        response = chat_llm.invoke(full_prompt)
                        answer = response.content
                        st.markdown(answer)
                        st.session_state.messages.append({"role": "assistant", "content": answer})
        else:
            st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€åŒæœŸã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã€AIã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚")
else:
    st.info("clients_data ãƒ•ã‚©ãƒ«ãƒ€ã«PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…¥ã‚Œã¦ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€åŒæœŸã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")